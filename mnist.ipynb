{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e01f1a36-116f-45d7-8d3d-b4b6c6f1f16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "url = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\"\n",
    "urllib.request.urlretrieve(url, \"mnist.npz\")\n",
    "\n",
    "mnist = np.load(\"mnist.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17684589-b1b4-4c42-93ac-4431f5ae63df",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = mnist['x_train']\n",
    "x_test = mnist['x_test']\n",
    "y_train = mnist['y_train']\n",
    "y_test = mnist['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dadba44-937c-42c9-84cd-53da55f91735",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# layer_dims = [nx, h1, ... nL]\n",
    "def initialize_parameters(layer_dims):\n",
    "    params = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        params[f\"W{i}\"] = np.random.randn(layer_dims[i], layer_dims[i-1]) * np.sqrt(2./layer_dims[i-1])\n",
    "        params[f\"b{i}\"] = np.zeros((layer_dims[i], 1))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ae6f8fa-431a-4c51-8499-d57535228650",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# Activation functions\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z,axis=0,keepdims=True))\n",
    "    return expZ/np.sum(expZ,axis=0,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5225dd1b-5347-4e5d-82df-27e889844208",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# Forward pass\n",
    "def forward_propagation(A_prev, W, b, func):\n",
    "    Z = np.dot(W,A_prev) + b\n",
    "    if func == \"relu\":\n",
    "        A = relu(Z)\n",
    "    if func == \"softmax\":\n",
    "        A = softmax(Z)\n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20312a0e-434b-4fd2-885f-fa419a9e5ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "def lost(Y, Y_hat):\n",
    "    m = Y.shape[1]\n",
    "    loss = -1/m * np.sum(Y * np.log(Y_hat + 1e-8))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85928f0e-2c1b-46e5-937b-8da04cb59db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# Backward pass\n",
    "def backward_propagation(dZ,A_prev,W,Z_prev, m):\n",
    "    dW = 1/m * np.dot(dZ,A_prev.T)\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    if Z_prev is None:\n",
    "        dZ_prev = None\n",
    "    else:\n",
    "        dZ_prev = np.dot(W.T, dZ) * (Z_prev>0)\n",
    "    return dZ_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "038c5147-7cbf-41cd-b8ef-ddafb4d92a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "def update_parameters(dW,db, alpha, W, b):\n",
    "    W -= alpha*dW\n",
    "    b -= alpha*db\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32975756-5474-4aa3-859b-3c2a30469f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "def train_model(n, X, Y, learning_rate, parameters):\n",
    "    cache = {\"A0\":X}\n",
    "    # n - 1 relu activations\n",
    "    for i in range(1, n - 1):\n",
    "        cache[f\"A{i}\"], cache[f\"Z{i}\"] = forward_propagation(cache[f\"A{i-1}\"], parameters[f\"W{i}\"], parameters[f\"b{i}\"], \"relu\")\n",
    "\n",
    "    # 1 softmax activation (output layer)\n",
    "    cache[f\"A{n - 1}\"], cache[f\"Z{n - 1}\"] = forward_propagation(cache[f\"A{n - 2}\"], parameters[f\"W{n - 1}\"], parameters[f\"b{n - 1}\"], \"softmax\")\n",
    "\n",
    "    # dL/dZ = Y_hat - Y = A^[L] - Y\n",
    "    dZ = cache[f\"A{n-1}\"] - Y\n",
    "    \n",
    "    grads = {}\n",
    "    for i in range(n-1,0,-1):\n",
    "        if i==1:\n",
    "            Z_prev = None\n",
    "        else:\n",
    "            Z_prev = cache[f\"Z{i-1}\"]\n",
    "        dZ, grads[f\"dW{i}\"], grads[f\"db{i}\"] = backward_propagation(dZ, cache[f\"A{i-1}\"], parameters[f\"W{i}\"], Z_prev, Y.shape[1])\n",
    "\n",
    "    for i in range(1,n):\n",
    "        update_parameters(grads[f\"dW{i}\"], grads[f\"db{i}\"], learning_rate, parameters[f\"W{i}\"], parameters[f\"b{i}\"])\n",
    "    return cache[f\"A{n-1}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e55ad8a-1124-41f0-a9fc-d5000aa1bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "def test_model(n, X, parameters):\n",
    "    cache = {\"A0\":X}\n",
    "    # n - 1 relu activations\n",
    "    for i in range(1, n - 1):\n",
    "        cache[f\"A{i}\"], cache[f\"Z{i}\"] = forward_propagation(cache[f\"A{i-1}\"], parameters[f\"W{i}\"], parameters[f\"b{i}\"], \"relu\")\n",
    "\n",
    "    # 1 softmax activation (output layer)\n",
    "    cache[f\"A{n - 1}\"], cache[f\"Z{n - 1}\"] = forward_propagation(cache[f\"A{n - 2}\"], parameters[f\"W{n - 1}\"], parameters[f\"b{n - 1}\"], \"softmax\")\n",
    "    return cache[f\"A{n-1}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d0b3725-2101-4ff4-8833-46d7076051d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(Y_hat, Y):\n",
    "    predictions = np.argmax(Y_hat, axis=0)\n",
    "    # Get true labels\n",
    "    labels = np.argmax(Y, axis=0)\n",
    "    # Calculate percentage\n",
    "    accuracy = np.mean(predictions == labels) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e026e2e3-688b-4812-a7ea-dfdcdae6fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "def nn_engine(x_train, y_train, x_test, y_test, iterations, learning_rate, layer_dims):\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    \n",
    "    # Normalization\n",
    "    x_train = x_train / 255.\n",
    "    x_test = x_test / 255.\n",
    "        \n",
    "    X = x_train.reshape(x_train.shape[0], -1).T\n",
    "    Y = np.zeros((layer_dims[len(layer_dims) - 1],y_train.size))\n",
    "    Y[y_train.flatten(), np.arange(y_train.size)] = 1\n",
    "\n",
    "    Y_test = np.zeros((layer_dims[len(layer_dims)-1],y_test.size))\n",
    "    Y_test[y_test.flatten(),np.arange(y_test.size)]=1\n",
    "    X_test = x_test.reshape(x_test.shape[0],-1).T\n",
    "\n",
    "    m = X.shape[1]\n",
    "    batch_size = 32\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        permutation = np.random.permutation(m)\n",
    "        X_shuffled = X[:, permutation]\n",
    "        Y_shuffled = Y[:, permutation]\n",
    "        for j in range(0, m, batch_size):\n",
    "            end = min(j + batch_size, m)\n",
    "            x_batch = X_shuffled[:, j : end]\n",
    "            y_batch = Y_shuffled[:, j : end]\n",
    "            \n",
    "            Y_hat = train_model(len(layer_dims), x_batch, y_batch, learning_rate, parameters)\n",
    "        \n",
    "        # print(f\"{i}: loss = {lost(y_batch, Y_hat)}; accuracy = {get_accuracy(Y_hat,y_batch)}\")\n",
    "    # Just add X_test and Y_test to test the model.\n",
    "    result = test_model(len(layer_dims),X_test,parameters)\n",
    "    print(f\"Test: loss = {lost(Y_test, result)}; accuracy = {get_accuracy(result, Y_test)}\")\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b596e554-57e7-413c-914a-458c45fece82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: loss = 0.0769989685406087; accuracy = 97.86\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "model_params = nn_engine(x_train, y_train, x_test, y_test, 20, 0.01, [784, 512, 256, 128, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3db6359-28a3-4f0a-9598-24d58de5318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging.\n",
    "\n",
    "# above 0.05\n",
    "# above 0.1 [784, 392, 10] 0.2 got 90%\n",
    "# changing layer = [784, 500, 10] 0.2\n",
    "# above 0.2\n",
    "# above 0.4\n",
    "# above 0.8 had 93%\n",
    "# above 1.6 (it had 94.98 test accuracy)\n",
    "# tried 3.2 saw ossicalation for 13 - 22 range this is too high; need lower than 3.2 cause 12% accuracy\n",
    "# 1.6 < a < 3.2; tring 2.4 32-19-33-27 need to go lower; acccuray = 30%\n",
    "# 1.6 < a < 2.4 tring 2 (16-49-35-34-34-35) lower than 2 needed. accuracy = 35%\n",
    "# 1.6 < a < 2; tring 1.8 got 89% may be closer to 1.6 is better\n",
    "# 1.6 < a < 1.8; tring 1.7 (94.39) 1.6 was better \n",
    "# maybe 0.8 < a < 1.6; tring 1.2 (94.87) tring 1.4 (95.12) tring 1.5 ()\n",
    "\n",
    "# using batches:\n",
    "# nn_engine(x_train, y_train, x_test, y_test, 20, 0.01, [784, 256, 128, 64, 32, 16, 10]) -> Test: loss = 0.09361683529738042; accuracy = 97.6\n",
    "# nn_engine(x_train, y_train, x_test, y_test, 20, 0.01, [784, 512, 256, 128, 10]) -> Test: loss = 0.0769989685406087; accuracy = 97.86"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
